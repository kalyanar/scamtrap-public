seed: 42

data:
  sms_spam_dataset: "ucirvine/sms_spam"
  phishing_dataset: "ealvaradob/phishing-dataset"
  phishing_config: "combined"
  max_length: 128
  test_size: 0.15
  val_size: 0.15
  holdout_intents: ["crypto", "romance"]
  min_samples_per_intent: 20
  output_dir: "data/processed"

intent_labeling:
  method: "keyword"

augmentations:
  n_views: 2
  strategies:
    - homoglyph
    - leetspeak
    - random_spacing
    - synonym_swap
  application_prob: 0.5

model:
  encoder_name: "distilbert-base-uncased"
  pooling: "cls"
  proj_hidden_dim: 512
  proj_output_dim: 128
  proj_layers: 2
  freeze_encoder_layers: 0

loss:
  type: "supcon"
  temperature: 0.07
  base_temperature: 0.07
  contrast_mode: "all"

training:
  batch_size: 64
  gradient_accumulation_steps: 4
  epochs: 20
  lr: 2.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  fp16: true
  early_stopping_patience: 5
  checkpoint_dir: "checkpoints"
  log_dir: "logs"

evaluation:
  fewshot_fractions: [0.01, 0.05, 0.10, 1.0]
  fewshot_trials: 5
  retrieval_k: [1, 5, 10, 20]
  clustering_seeds: 10
  robustness_augmentations_per_sample: 5
  results_dir: "results"

baselines:
  tfidf:
    max_features: 10000
    ngram_range: [1, 3]
  finetuned_bert:
    encoder_name: "distilbert-base-uncased"
    epochs: 10
    lr: 2.0e-5
  sbert:
    model_name: "all-MiniLM-L6-v2"

# Stage B: CLIP-style intent alignment
stage_b:
  proj_dim: 256
  initial_temperature: 0.07
  batch_size: 64
  gradient_accumulation_steps: 2
  epochs: 15
  lr: 2.0e-5
  warmup_ratio: 0.1
  fp16: true
  early_stopping_patience: 5
  checkpoint_dir: "checkpoints/clip"
  message_encoder_init: "stage_a"  # "stage_a" or "pretrained"
  freeze_description_encoder: true  # Freeze desc encoder to preserve pretrained semantics for zero-shot
  freeze_description_layers: -1  # -1=use bool above, 0-6=freeze bottom N layers (top unfrozen)
  l2sp_alpha: 0.0  # L2-SP regularization strength (0=disabled)

# Stage C: World model for trajectory prediction
stage_c:
  conversation_dataset: "BothBosu/multi-agent-scam-conversation"
  max_turns: 30
  num_stages: 6
  gru_hidden_dim: 256
  gru_layers: 2
  gru_dropout: 0.2
  transformer_heads: 4
  transformer_layers: 2
  batch_size: 32
  epochs: 50
  lr: 1.0e-3
  weight_decay: 0.01
  warmup_ratio: 0.05
  early_stopping_patience: 10
  loss_alpha: 0.7
  checkpoint_dir: "checkpoints/world_model"
  robustness:
    perturbation_levels: [0.0, 0.1, 0.2, 0.3, 0.5]
    n_trials: 5
